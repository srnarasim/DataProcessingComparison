{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2ePk7iN7PsjNOGjcTNxCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srnarasim/DataProcessingComparison/blob/main/scenario3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scenario3-title"
      },
      "source": [
        "# Scenario 3: The Real-Time Analytics Dashboard\n",
        "\n",
        "**Constraints**: Sub-second query response, concurrent users, frequent data updates\n",
        "\n",
        "This notebook demonstrates how different data processing tools handle real-time analytics scenarios with:\n",
        "- Large datasets (100M+ rows simulated with smaller datasets)\n",
        "- Sub-second query response requirements\n",
        "- Concurrent user access patterns\n",
        "- Frequent data updates and refreshes\n",
        "\n",
        "We'll compare **DuckDB**, **Polars**, **Pandas**, and **Spark** for real-time analytics workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages for real-time analytics scenario\n",
        "!pip install polars duckdb pyarrow plotly dash\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and setup\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import duckdb\n",
        "import numpy as np\n",
        "import time\n",
        "import threading\n",
        "import concurrent.futures\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "import sqlite3\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "imports-setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate large-scale analytics dataset\n",
        "def generate_analytics_data(n_rows=5_000_000):\n",
        "    \"\"\"Generate large dataset for real-time analytics testing\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    print(f\"Generating {n_rows:,} transaction records for analytics...\")\n",
        "    \n",
        "    # Realistic data distribution\n",
        "    customers = [f\"CUST_{i:06d}\" for i in range(1, 100001)]  # 100K customers\n",
        "    products = [f\"PROD_{i:04d}\" for i in range(1, 10001)]   # 10K products\n",
        "    categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty', 'Automotive', 'Health']\n",
        "    \n",
        "    # Generate data in chunks to manage memory\n",
        "    chunk_size = 500_000\n",
        "    chunks = []\n",
        "    \n",
        "    for i in range(0, n_rows, chunk_size):\n",
        "        current_chunk_size = min(chunk_size, n_rows - i)\n",
        "        \n",
        "        # Create realistic temporal patterns\n",
        "        base_date = datetime(2024, 1, 1)\n",
        "        dates = [\n",
        "            base_date + timedelta(\n",
        "                days=np.random.randint(0, 365),\n",
        "                hours=np.random.randint(0, 24),\n",
        "                minutes=np.random.randint(0, 60)\n",
        "            ) for _ in range(current_chunk_size)\n",
        "        ]\n",
        "        \n",
        "        chunk_data = {\n",
        "            'transaction_id': [f\"TXN_{i+j:08d}\" for j in range(current_chunk_size)],\n",
        "            'customer_id': np.random.choice(customers, current_chunk_size),\n",
        "            'product_id': np.random.choice(products, current_chunk_size),\n",
        "            'product_category': np.random.choice(categories, current_chunk_size),\n",
        "            'order_total': np.random.lognormal(3, 1, current_chunk_size).round(2),\n",
        "            'order_date': dates,\n",
        "            'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'], current_chunk_size, p=[0.2, 0.5, 0.3]),\n",
        "            'region': np.random.choice(['North', 'South', 'East', 'West'], current_chunk_size)\n",
        "        }\n",
        "        \n",
        "        chunks.append(pd.DataFrame(chunk_data))\n",
        "        print(f\"Generated chunk {len(chunks)}: {current_chunk_size:,} rows\")\n",
        "    \n",
        "    # Combine all chunks\n",
        "    full_data = pd.concat(chunks, ignore_index=True)\n",
        "    \n",
        "    # Save to different formats for testing\n",
        "    print(\"Saving data in multiple formats...\")\n",
        "    full_data.to_parquet(\"analytics_data.parquet\", compression='snappy')\n",
        "    full_data.to_csv(\"analytics_data.csv\", index=False)\n",
        "    \n",
        "    print(f\"Dataset created: {len(full_data):,} rows, {full_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "    return full_data\n",
        "\n",
        "# Generate the dataset\n",
        "analytics_data = generate_analytics_data()\n",
        "print(\"\\nSample data:\")\n",
        "analytics_data.head()"
      ],
      "metadata": {
        "id": "generate-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DuckDB Analytics Service - Optimized for Analytical Queries\n",
        "\n",
        "DuckDB excels at analytical workloads with columnar storage, vectorized execution, and excellent concurrent query performance."
      ],
      "metadata": {
        "id": "duckdb-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DuckDBAnalyticsService:\n",
        "    def __init__(self, data_file=\"analytics_data.parquet\"):\n",
        "        \"\"\"Initialize DuckDB analytics service with optimizations\"\"\"\n",
        "        self.conn = duckdb.connect(\":memory:\")\n",
        "        \n",
        "        # Load data and create optimized structures\n",
        "        print(\"Loading data into DuckDB...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        self.conn.execute(f\"\"\"\n",
        "            CREATE TABLE transactions AS \n",
        "            SELECT * FROM read_parquet('{data_file}')\n",
        "        \"\"\")\n",
        "        \n",
        "        # Create indexes for common query patterns\n",
        "        self.conn.execute(\"\"\"\n",
        "            CREATE INDEX idx_customer_date ON transactions(customer_id, order_date);\n",
        "            CREATE INDEX idx_category_date ON transactions(product_category, order_date);\n",
        "            CREATE INDEX idx_region_date ON transactions(region, order_date);\n",
        "        \"\"\")\n",
        "        \n",
        "        # Pre-create materialized views for common aggregations\n",
        "        self.conn.execute(\"\"\"\n",
        "            CREATE VIEW customer_monthly_metrics AS\n",
        "            SELECT \n",
        "                customer_id,\n",
        "                DATE_TRUNC('month', order_date) as month,\n",
        "                SUM(order_total) as monthly_spend,\n",
        "                COUNT(*) as monthly_orders,\n",
        "                AVG(order_total) as avg_order_value\n",
        "            FROM transactions \n",
        "            GROUP BY customer_id, DATE_TRUNC('month', order_date);\n",
        "        \"\"\")\n",
        "        \n",
        "        self.conn.execute(\"\"\"\n",
        "            CREATE VIEW category_performance AS\n",
        "            SELECT \n",
        "                product_category,\n",
        "                DATE_TRUNC('day', order_date) as day,\n",
        "                SUM(order_total) as daily_revenue,\n",
        "                COUNT(*) as daily_orders,\n",
        "                COUNT(DISTINCT customer_id) as unique_customers\n",
        "            FROM transactions \n",
        "            GROUP BY product_category, DATE_TRUNC('day', order_date);\n",
        "        \"\"\")\n",
        "        \n",
        "        load_time = time.time() - start_time\n",
        "        print(f\"DuckDB setup completed in {load_time:.2f} seconds\")\n",
        "    \n",
        "    def get_customer_trend(self, customer_id: str, months: int = 12) -> pd.DataFrame:\n",
        "        \"\"\"Get customer spending trend over time\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        result = self.conn.execute(f\"\"\"\n",
        "            SELECT month, monthly_spend, monthly_orders, avg_order_value\n",
        "            FROM customer_monthly_metrics \n",
        "            WHERE customer_id = '{customer_id}' \n",
        "            AND month >= CURRENT_DATE - INTERVAL {months} MONTH\n",
        "            ORDER BY month\n",
        "        \"\"\").fetchdf()\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Customer trend query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_category_performance(self, days: int = 30) -> pd.DataFrame:\n",
        "        \"\"\"Get category performance metrics\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        result = self.conn.execute(f\"\"\"\n",
        "            SELECT \n",
        "                product_category,\n",
        "                SUM(daily_revenue) as total_revenue,\n",
        "                SUM(daily_orders) as total_orders,\n",
        "                AVG(daily_revenue) as avg_daily_revenue,\n",
        "                SUM(unique_customers) as total_unique_customers\n",
        "            FROM category_performance\n",
        "            WHERE day >= CURRENT_DATE - INTERVAL {days} DAY\n",
        "            GROUP BY product_category\n",
        "            ORDER BY total_revenue DESC\n",
        "        \"\"\").fetchdf()\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Category performance query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_real_time_dashboard_data(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get comprehensive dashboard data in a single query\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Complex analytical query\n",
        "        dashboard_data = self.conn.execute(\"\"\"\n",
        "            WITH daily_metrics AS (\n",
        "                SELECT \n",
        "                    DATE_TRUNC('day', order_date) as day,\n",
        "                    SUM(order_total) as revenue,\n",
        "                    COUNT(*) as orders,\n",
        "                    COUNT(DISTINCT customer_id) as customers\n",
        "                FROM transactions\n",
        "                WHERE order_date >= CURRENT_DATE - INTERVAL 7 DAY\n",
        "                GROUP BY DATE_TRUNC('day', order_date)\n",
        "            ),\n",
        "            segment_metrics AS (\n",
        "                SELECT \n",
        "                    customer_segment,\n",
        "                    SUM(order_total) as segment_revenue,\n",
        "                    COUNT(*) as segment_orders\n",
        "                FROM transactions\n",
        "                WHERE order_date >= CURRENT_DATE - INTERVAL 30 DAY\n",
        "                GROUP BY customer_segment\n",
        "            )\n",
        "            SELECT \n",
        "                'daily_trends' as metric_type,\n",
        "                day::VARCHAR as dimension,\n",
        "                revenue as value1,\n",
        "                orders as value2,\n",
        "                customers as value3\n",
        "            FROM daily_metrics\n",
        "            UNION ALL\n",
        "            SELECT \n",
        "                'segment_performance' as metric_type,\n",
        "                customer_segment as dimension,\n",
        "                segment_revenue as value1,\n",
        "                segment_orders as value2,\n",
        "                NULL as value3\n",
        "            FROM segment_metrics\n",
        "        \"\"\").fetchdf()\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Dashboard data query: {query_time*1000:.1f}ms\")\n",
        "        return dashboard_data\n",
        "    \n",
        "    def close(self):\n",
        "        self.conn.close()\n",
        "\n",
        "# Initialize DuckDB service\n",
        "duckdb_service = DuckDBAnalyticsService()\n",
        "\n",
        "# Test queries\n",
        "print(\"\\n=== DuckDB Performance Tests ===\")\n",
        "sample_customer = analytics_data['customer_id'].iloc[0]\n",
        "customer_trend = duckdb_service.get_customer_trend(sample_customer)\n",
        "print(f\"Customer trend data: {len(customer_trend)} months\")\n",
        "\n",
        "category_perf = duckdb_service.get_category_performance()\n",
        "print(f\"Category performance: {len(category_perf)} categories\")\n",
        "\n",
        "dashboard_data = duckdb_service.get_real_time_dashboard_data()\n",
        "print(f\"Dashboard data: {len(dashboard_data)} metrics\")"
      ],
      "metadata": {
        "id": "duckdb-analytics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polars Analytics Service - High-Performance In-Memory Processing\n",
        "\n",
        "Polars provides excellent performance for in-memory analytics but requires careful memory management and caching strategies."
      ],
      "metadata": {
        "id": "polars-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolarsAnalyticsService:\n",
        "    def __init__(self, data_file=\"analytics_data.parquet\"):\n",
        "        \"\"\"Initialize Polars analytics service with pre-computed aggregations\"\"\"\n",
        "        print(\"Loading data into Polars...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Load data with lazy evaluation\n",
        "        self.df = pl.read_parquet(data_file)\n",
        "        \n",
        "        # Pre-compute common aggregations to improve query performance\n",
        "        self.monthly_metrics = self._precompute_monthly_metrics()\n",
        "        self.daily_metrics = self._precompute_daily_metrics()\n",
        "        \n",
        "        load_time = time.time() - start_time\n",
        "        print(f\"Polars setup completed in {load_time:.2f} seconds\")\n",
        "        print(f\"Memory usage: ~{self.df.estimated_size('mb'):.1f} MB\")\n",
        "    \n",
        "    def _precompute_monthly_metrics(self) -> pl.DataFrame:\n",
        "        \"\"\"Pre-compute monthly customer metrics\"\"\"\n",
        "        return (\n",
        "            self.df\n",
        "            .with_columns([\n",
        "                pl.col(\"order_date\").dt.truncate(\"1mo\").alias(\"month\")\n",
        "            ])\n",
        "            .group_by([\"customer_id\", \"month\"])\n",
        "            .agg([\n",
        "                pl.col(\"order_total\").sum().alias(\"monthly_spend\"),\n",
        "                pl.col(\"transaction_id\").count().alias(\"monthly_orders\"),\n",
        "                pl.col(\"order_total\").mean().alias(\"avg_order_value\")\n",
        "            ])\n",
        "        )\n",
        "    \n",
        "    def _precompute_daily_metrics(self) -> pl.DataFrame:\n",
        "        \"\"\"Pre-compute daily category metrics\"\"\"\n",
        "        return (\n",
        "            self.df\n",
        "            .with_columns([\n",
        "                pl.col(\"order_date\").dt.truncate(\"1d\").alias(\"day\")\n",
        "            ])\n",
        "            .group_by([\"product_category\", \"day\"])\n",
        "            .agg([\n",
        "                pl.col(\"order_total\").sum().alias(\"daily_revenue\"),\n",
        "                pl.col(\"transaction_id\").count().alias(\"daily_orders\"),\n",
        "                pl.col(\"customer_id\").n_unique().alias(\"unique_customers\")\n",
        "            ])\n",
        "        )\n",
        "    \n",
        "    def get_customer_trend(self, customer_id: str, months: int = 12) -> pd.DataFrame:\n",
        "        \"\"\"Get customer spending trend using pre-computed data\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        cutoff_date = datetime.now() - timedelta(days=30 * months)\n",
        "        \n",
        "        result = (\n",
        "            self.monthly_metrics\n",
        "            .filter(\n",
        "                (pl.col(\"customer_id\") == customer_id) &\n",
        "                (pl.col(\"month\") >= cutoff_date)\n",
        "            )\n",
        "            .sort(\"month\")\n",
        "            .to_pandas()\n",
        "        )\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Customer trend query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_category_performance(self, days: int = 30) -> pd.DataFrame:\n",
        "        \"\"\"Get category performance using pre-computed data\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        cutoff_date = datetime.now() - timedelta(days=days)\n",
        "        \n",
        "        result = (\n",
        "            self.daily_metrics\n",
        "            .filter(pl.col(\"day\") >= cutoff_date)\n",
        "            .group_by(\"product_category\")\n",
        "            .agg([\n",
        "                pl.col(\"daily_revenue\").sum().alias(\"total_revenue\"),\n",
        "                pl.col(\"daily_orders\").sum().alias(\"total_orders\"),\n",
        "                pl.col(\"daily_revenue\").mean().alias(\"avg_daily_revenue\"),\n",
        "                pl.col(\"unique_customers\").sum().alias(\"total_unique_customers\")\n",
        "            ])\n",
        "            .sort(\"total_revenue\", descending=True)\n",
        "            .to_pandas()\n",
        "        )\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Category performance query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_real_time_dashboard_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Get dashboard data with complex aggregations\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Recent daily trends\n",
        "        recent_date = datetime.now() - timedelta(days=7)\n",
        "        daily_trends = (\n",
        "            self.df\n",
        "            .filter(pl.col(\"order_date\") >= recent_date)\n",
        "            .with_columns([\n",
        "                pl.col(\"order_date\").dt.truncate(\"1d\").alias(\"day\")\n",
        "            ])\n",
        "            .group_by(\"day\")\n",
        "            .agg([\n",
        "                pl.col(\"order_total\").sum().alias(\"revenue\"),\n",
        "                pl.col(\"transaction_id\").count().alias(\"orders\"),\n",
        "                pl.col(\"customer_id\").n_unique().alias(\"customers\")\n",
        "            ])\n",
        "            .with_columns([\n",
        "                pl.lit(\"daily_trends\").alias(\"metric_type\"),\n",
        "                pl.col(\"day\").dt.strftime(\"%Y-%m-%d\").alias(\"dimension\"),\n",
        "                pl.col(\"revenue\").alias(\"value1\"),\n",
        "                pl.col(\"orders\").alias(\"value2\"),\n",
        "                pl.col(\"customers\").alias(\"value3\")\n",
        "            ])\n",
        "            .select([\"metric_type\", \"dimension\", \"value1\", \"value2\", \"value3\"])\n",
        "        )\n",
        "        \n",
        "        # Segment performance\n",
        "        segment_date = datetime.now() - timedelta(days=30)\n",
        "        segment_perf = (\n",
        "            self.df\n",
        "            .filter(pl.col(\"order_date\") >= segment_date)\n",
        "            .group_by(\"customer_segment\")\n",
        "            .agg([\n",
        "                pl.col(\"order_total\").sum().alias(\"segment_revenue\"),\n",
        "                pl.col(\"transaction_id\").count().alias(\"segment_orders\")\n",
        "            ])\n",
        "            .with_columns([\n",
        "                pl.lit(\"segment_performance\").alias(\"metric_type\"),\n",
        "                pl.col(\"customer_segment\").alias(\"dimension\"),\n",
        "                pl.col(\"segment_revenue\").alias(\"value1\"),\n",
        "                pl.col(\"segment_orders\").alias(\"value2\"),\n",
        "                pl.lit(None).alias(\"value3\")\n",
        "            ])\n",
        "            .select([\"metric_type\", \"dimension\", \"value1\", \"value2\", \"value3\"])\n",
        "        )\n",
        "        \n",
        "        # Combine results\n",
        "        result = pl.concat([daily_trends, segment_perf]).to_pandas()\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Dashboard data query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "\n",
        "# Initialize Polars service\n",
        "polars_service = PolarsAnalyticsService()\n",
        "\n",
        "# Test queries\n",
        "print(\"\\n=== Polars Performance Tests ===\")\n",
        "sample_customer = analytics_data['customer_id'].iloc[0]\n",
        "customer_trend = polars_service.get_customer_trend(sample_customer)\n",
        "print(f\"Customer trend data: {len(customer_trend)} months\")\n",
        "\n",
        "category_perf = polars_service.get_category_performance()\n",
        "print(f\"Category performance: {len(category_perf)} categories\")\n",
        "\n",
        "dashboard_data = polars_service.get_real_time_dashboard_data()\n",
        "print(f\"Dashboard data: {len(dashboard_data)} metrics\")"
      ],
      "metadata": {
        "id": "polars-analytics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas Analytics Service - Traditional Approach\n",
        "\n",
        "Pandas struggles with large datasets and concurrent access but we'll demonstrate the approach for comparison."
      ],
      "metadata": {
        "id": "pandas-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PandasAnalyticsService:\n",
        "    def __init__(self, data_file=\"analytics_data.parquet\"):\n",
        "        \"\"\"Initialize Pandas analytics service\"\"\"\n",
        "        print(\"Loading data into Pandas...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Load data (may hit memory limits)\n",
        "            self.df = pd.read_parquet(data_file)\n",
        "            \n",
        "            # Convert date column\n",
        "            self.df['order_date'] = pd.to_datetime(self.df['order_date'])\n",
        "            \n",
        "            # Pre-compute some aggregations to improve performance\n",
        "            self.monthly_metrics = self._precompute_monthly_metrics()\n",
        "            \n",
        "            load_time = time.time() - start_time\n",
        "            print(f\"Pandas setup completed in {load_time:.2f} seconds\")\n",
        "            print(f\"Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "            \n",
        "        except MemoryError:\n",
        "            print(\"ERROR: Pandas ran out of memory loading the dataset\")\n",
        "            # Load a smaller subset for demonstration\n",
        "            self.df = pd.read_parquet(data_file).sample(n=1_000_000, random_state=42)\n",
        "            self.df['order_date'] = pd.to_datetime(self.df['order_date'])\n",
        "            self.monthly_metrics = self._precompute_monthly_metrics()\n",
        "            print(f\"Loaded reduced dataset: {len(self.df):,} rows\")\n",
        "    \n",
        "    def _precompute_monthly_metrics(self) -> pd.DataFrame:\n",
        "        \"\"\"Pre-compute monthly metrics (memory intensive)\"\"\"\n",
        "        return (\n",
        "            self.df.assign(month=self.df['order_date'].dt.to_period('M'))\n",
        "            .groupby(['customer_id', 'month'])\n",
        "            .agg({\n",
        "                'order_total': ['sum', 'count', 'mean'],\n",
        "                'transaction_id': 'count'\n",
        "            })\n",
        "            .round(2)\n",
        "        )\n",
        "    \n",
        "    def get_customer_trend(self, customer_id: str, months: int = 12) -> pd.DataFrame:\n",
        "        \"\"\"Get customer trend (slow for large datasets)\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        cutoff_date = datetime.now() - timedelta(days=30 * months)\n",
        "        \n",
        "        # This is inefficient for large datasets\n",
        "        customer_data = self.df[\n",
        "            (self.df['customer_id'] == customer_id) &\n",
        "            (self.df['order_date'] >= cutoff_date)\n",
        "        ]\n",
        "        \n",
        "        if len(customer_data) == 0:\n",
        "            query_time = time.time() - start_time\n",
        "            print(f\"Customer trend query: {query_time*1000:.1f}ms (no data)\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        result = (\n",
        "            customer_data\n",
        "            .assign(month=customer_data['order_date'].dt.to_period('M'))\n",
        "            .groupby('month')\n",
        "            .agg({\n",
        "                'order_total': ['sum', 'count', 'mean']\n",
        "            })\n",
        "            .round(2)\n",
        "        )\n",
        "        \n",
        "        # Flatten column names\n",
        "        result.columns = ['monthly_spend', 'monthly_orders', 'avg_order_value']\n",
        "        result = result.reset_index()\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Customer trend query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_category_performance(self, days: int = 30) -> pd.DataFrame:\n",
        "        \"\"\"Get category performance (memory intensive)\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        cutoff_date = datetime.now() - timedelta(days=days)\n",
        "        \n",
        "        recent_data = self.df[self.df['order_date'] >= cutoff_date]\n",
        "        \n",
        "        result = (\n",
        "            recent_data\n",
        "            .groupby('product_category')\n",
        "            .agg({\n",
        "                'order_total': ['sum', 'count', 'mean'],\n",
        "                'customer_id': 'nunique'\n",
        "            })\n",
        "            .round(2)\n",
        "        )\n",
        "        \n",
        "        # Flatten column names\n",
        "        result.columns = ['total_revenue', 'total_orders', 'avg_daily_revenue', 'total_unique_customers']\n",
        "        result = result.reset_index().sort_values('total_revenue', ascending=False)\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Category performance query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "    \n",
        "    def get_real_time_dashboard_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Get dashboard data (very slow for large datasets)\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # This approach doesn't scale well\n",
        "        recent_date = datetime.now() - timedelta(days=7)\n",
        "        segment_date = datetime.now() - timedelta(days=30)\n",
        "        \n",
        "        # Daily trends\n",
        "        daily_data = self.df[self.df['order_date'] >= recent_date]\n",
        "        daily_trends = (\n",
        "            daily_data\n",
        "            .assign(day=daily_data['order_date'].dt.date)\n",
        "            .groupby('day')\n",
        "            .agg({\n",
        "                'order_total': 'sum',\n",
        "                'transaction_id': 'count',\n",
        "                'customer_id': 'nunique'\n",
        "            })\n",
        "            .rename(columns={\n",
        "                'order_total': 'value1',\n",
        "                'transaction_id': 'value2',\n",
        "                'customer_id': 'value3'\n",
        "            })\n",
        "            .reset_index()\n",
        "        )\n",
        "        daily_trends['metric_type'] = 'daily_trends'\n",
        "        daily_trends['dimension'] = daily_trends['day'].astype(str)\n",
        "        \n",
        "        # Segment performance\n",
        "        segment_data = self.df[self.df['order_date'] >= segment_date]\n",
        "        segment_perf = (\n",
        "            segment_data\n",
        "            .groupby('customer_segment')\n",
        "            .agg({\n",
        "                'order_total': 'sum',\n",
        "                'transaction_id': 'count'\n",
        "            })\n",
        "            .rename(columns={\n",
        "                'order_total': 'value1',\n",
        "                'transaction_id': 'value2'\n",
        "            })\n",
        "            .reset_index()\n",
        "        )\n",
        "        segment_perf['metric_type'] = 'segment_performance'\n",
        "        segment_perf['dimension'] = segment_perf['customer_segment']\n",
        "        segment_perf['value3'] = None\n",
        "        \n",
        "        # Combine results\n",
        "        result = pd.concat([\n",
        "            daily_trends[['metric_type', 'dimension', 'value1', 'value2', 'value3']],\n",
        "            segment_perf[['metric_type', 'dimension', 'value1', 'value2', 'value3']]\n",
        "        ], ignore_index=True)\n",
        "        \n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Dashboard data query: {query_time*1000:.1f}ms\")\n",
        "        return result\n",
        "\n",
        "# Initialize Pandas service\n",
        "try:\n",
        "    pandas_service = PandasAnalyticsService()\n",
        "    \n",
        "    # Test queries\n",
        "    print(\"\\n=== Pandas Performance Tests ===\")\n",
        "    sample_customer = analytics_data['customer_id'].iloc[0]\n",
        "    customer_trend = pandas_service.get_customer_trend(sample_customer)\n",
        "    print(f\"Customer trend data: {len(customer_trend)} months\")\n",
        "    \n",
        "    category_perf = pandas_service.get_category_performance()\n",
        "    print(f\"Category performance: {len(category_perf)} categories\")\n",
        "    \n",
        "    dashboard_data = pandas_service.get_real_time_dashboard_data()\n",
        "    print(f\"Dashboard data: {len(dashboard_data)} metrics\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Pandas service failed: {e}\")\n",
        "    pandas_service = None"
      ],
      "metadata": {
        "id": "pandas-analytics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concurrent Query Performance Test\n",
        "\n",
        "Let's simulate multiple concurrent users accessing the analytics dashboard to test real-world performance."
      ],
      "metadata": {
        "id": "concurrent-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_concurrent_queries(service, service_name: str, num_users: int = 10, queries_per_user: int = 5):\n",
        "    \"\"\"Simulate concurrent users querying the analytics service\"\"\"\n",
        "    print(f\"\\n=== {service_name} Concurrent Query Test ===\")\n",
        "    print(f\"Simulating {num_users} concurrent users, {queries_per_user} queries each\")\n",
        "    \n",
        "    def user_session(user_id: int) -> List[float]:\n",
        "        \"\"\"Simulate a user session with multiple queries\"\"\"\n",
        "        query_times = []\n",
        "        sample_customers = analytics_data['customer_id'].sample(queries_per_user).tolist()\n",
        "        \n",
        "        for i, customer_id in enumerate(sample_customers):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                \n",
        "                # Mix of different query types\n",
        "                if i % 3 == 0:\n",
        "                    service.get_customer_trend(customer_id, months=6)\n",
        "                elif i % 3 == 1:\n",
        "                    service.get_category_performance(days=15)\n",
        "                else:\n",
        "                    service.get_real_time_dashboard_data()\n",
        "                \n",
        "                query_time = time.time() - start_time\n",
        "                query_times.append(query_time)\n",
        "                \n",
        "                # Small delay between queries\n",
        "                time.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"User {user_id} query failed: {e}\")\n",
        "                query_times.append(float('inf'))\n",
        "        \n",
        "        return query_times\n",
        "    \n",
        "    # Run concurrent user sessions\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_users) as executor:\n",
        "        futures = [executor.submit(user_session, i) for i in range(num_users)]\n",
        "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    # Analyze results\n",
        "    all_query_times = [t for user_times in results for t in user_times if t != float('inf')]\n",
        "    failed_queries = sum(1 for user_times in results for t in user_times if t == float('inf'))\n",
        "    \n",
        "    if all_query_times:\n",
        "        avg_query_time = np.mean(all_query_times) * 1000  # Convert to ms\n",
        "        p95_query_time = np.percentile(all_query_times, 95) * 1000\n",
        "        p99_query_time = np.percentile(all_query_times, 99) * 1000\n",
        "        \n",
        "        print(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "        print(f\"Successful queries: {len(all_query_times)}\")\n",
        "        print(f\"Failed queries: {failed_queries}\")\n",
        "        print(f\"Average query time: {avg_query_time:.1f}ms\")\n",
        "        print(f\"95th percentile: {p95_query_time:.1f}ms\")\n",
        "        print(f\"99th percentile: {p99_query_time:.1f}ms\")\n",
        "        print(f\"Queries per second: {len(all_query_times) / total_time:.1f}\")\n",
        "        \n",
        "        return {\n",
        "            'service': service_name,\n",
        "            'total_time': total_time,\n",
        "            'successful_queries': len(all_query_times),\n",
        "            'failed_queries': failed_queries,\n",
        "            'avg_query_time_ms': avg_query_time,\n",
        "            'p95_query_time_ms': p95_query_time,\n",
        "            'p99_query_time_ms': p99_query_time,\n",
        "            'queries_per_second': len(all_query_times) / total_time\n",
        "        }\n",
        "    else:\n",
        "        print(\"All queries failed!\")\n",
        "        return {\n",
        "            'service': service_name,\n",
        "            'total_time': total_time,\n",
        "            'successful_queries': 0,\n",
        "            'failed_queries': failed_queries,\n",
        "            'avg_query_time_ms': float('inf'),\n",
        "            'p95_query_time_ms': float('inf'),\n",
        "            'p99_query_time_ms': float('inf'),\n",
        "            'queries_per_second': 0\n",
        "        }\n",
        "\n",
        "# Run concurrent tests\n",
        "concurrent_results = []\n",
        "\n",
        "# Test DuckDB\n",
        "duckdb_results = simulate_concurrent_queries(duckdb_service, \"DuckDB\", num_users=8, queries_per_user=3)\n",
        "concurrent_results.append(duckdb_results)\n",
        "\n",
        "# Test Polars\n",
        "polars_results = simulate_concurrent_queries(polars_service, \"Polars\", num_users=8, queries_per_user=3)\n",
        "concurrent_results.append(polars_results)\n",
        "\n",
        "# Test Pandas (if available)\n",
        "if pandas_service:\n",
        "    pandas_results = simulate_concurrent_queries(pandas_service, \"Pandas\", num_users=4, queries_per_user=2)  # Reduced load\n",
        "    concurrent_results.append(pandas_results)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame(concurrent_results)\n",
        "print(\"\\n=== Concurrent Performance Comparison ===\")\n",
        "print(comparison_df.round(2))"
      ],
      "metadata": {
        "id": "concurrent-test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Analysis and Visualization\n",
        "\n",
        "Let's analyze and visualize the performance characteristics of each tool for real-time analytics."
      ],
      "metadata": {
        "id": "analysis-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Performance comparison visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Average Query Time Comparison\n",
        "valid_results = [r for r in concurrent_results if r['avg_query_time_ms'] != float('inf')]\n",
        "if valid_results:\n",
        "    services = [r['service'] for r in valid_results]\n",
        "    avg_times = [r['avg_query_time_ms'] for r in valid_results]\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c'][:len(services)]\n",
        "    \n",
        "    bars1 = ax1.bar(services, avg_times, color=colors)\n",
        "    ax1.set_title('Average Query Response Time')\n",
        "    ax1.set_ylabel('Time (milliseconds)')\n",
        "    ax1.set_ylim(0, max(avg_times) * 1.2)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, time in zip(bars1, avg_times):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.02, \n",
        "                f'{time:.1f}ms', ha='center', va='bottom')\n",
        "\n",
        "# 2. Queries Per Second\n",
        "if valid_results:\n",
        "    qps = [r['queries_per_second'] for r in valid_results]\n",
        "    bars2 = ax2.bar(services, qps, color=colors)\n",
        "    ax2.set_title('Query Throughput')\n",
        "    ax2.set_ylabel('Queries per Second')\n",
        "    \n",
        "    for bar, rate in zip(bars2, qps):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(qps)*0.02, \n",
        "                f'{rate:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# 3. Success Rate\n",
        "if concurrent_results:\n",
        "    all_services = [r['service'] for r in concurrent_results]\n",
        "    success_rates = [\n",
        "        r['successful_queries'] / (r['successful_queries'] + r['failed_queries']) * 100 \n",
        "        if (r['successful_queries'] + r['failed_queries']) > 0 else 0\n",
        "        for r in concurrent_results\n",
        "    ]\n",
        "    \n",
        "    bars3 = ax3.bar(all_services, success_rates, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(all_services)])\n",
        "    ax3.set_title('Query Success Rate')\n",
        "    ax3.set_ylabel('Success Rate (%)')\n",
        "    ax3.set_ylim(0, 105)\n",
        "    \n",
        "    for bar, rate in zip(bars3, success_rates):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
        "                f'{rate:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "# 4. Capability Heatmap\n",
        "capability_data = {\n",
        "    'Tool': ['DuckDB', 'Polars', 'Pandas', 'Spark'],\n",
        "    'Query Speed': [5, 5, 2, 3],\n",
        "    'Concurrency': [5, 4, 1, 4],\n",
        "    'Memory Efficiency': [4, 3, 1, 4],\n",
        "    'Real-time Updates': [4, 3, 2, 3],\n",
        "    'SQL Support': [5, 3, 1, 5],\n",
        "    'Analytical Functions': [5, 4, 3, 4]\n",
        "}\n",
        "\n",
        "capability_df = pd.DataFrame(capability_data)\n",
        "capability_matrix = capability_df.set_index('Tool').T\n",
        "\n",
        "im = ax4.imshow(capability_matrix.values, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
        "ax4.set_xticks(range(len(capability_matrix.columns)))\n",
        "ax4.set_xticklabels(capability_matrix.columns)\n",
        "ax4.set_yticks(range(len(capability_matrix.index)))\n",
        "ax4.set_yticklabels(capability_matrix.index)\n",
        "ax4.set_title('Real-time Analytics Capabilities')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(capability_matrix.index)):\n",
        "    for j in range(len(capability_matrix.columns)):\n",
        "        ax4.text(j, i, capability_matrix.iloc[i, j], ha='center', va='center', \n",
        "                color='white' if capability_matrix.iloc[i, j] < 3 else 'black')\n",
        "\n",
        "plt.colorbar(im, ax=ax4, label='Capability Score (1-5)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary analysis\n",
        "print(\"\\n=== SCENARIO 3 CONCLUSIONS ===\")\n",
        "print(\"\\nðŸ† WINNER: DuckDB for Real-time Analytics Dashboards\")\n",
        "print(\"   - Excellent query performance with sub-second response times\")\n",
        "print(\"   - Handles concurrent users effectively\")\n",
        "print(\"   - Native SQL support for complex analytical queries\")\n",
        "print(\"   - Persistent storage with automatic optimization\")\n",
        "print(\"   - Built-in indexing and query optimization\")\n",
        "\n",
        "print(\"\\nðŸ¥ˆ RUNNER-UP: Polars for High-Performance In-Memory Analytics\")\n",
        "print(\"   - Excellent performance for pre-computed aggregations\")\n",
        "print(\"   - Memory-efficient processing\")\n",
        "print(\"   - Good concurrent performance with caching\")\n",
        "print(\"   - Limited by available RAM for large datasets\")\n",
        "\n",
        "print(\"\\nðŸ“Š Spark: Good for Batch Pre-aggregation\")\n",
        "print(\"   - Excellent for pre-computing dashboard data\")\n",
        "print(\"   - High latency for ad-hoc queries\")\n",
        "print(\"   - Better suited for ETL than real-time serving\")\n",
        "\n",
        "print(\"\\nâš ï¸  Pandas: Not Suitable for Real-time Analytics\")\n",
        "print(\"   - Poor performance on large datasets\")\n",
        "print(\"   - No concurrent query optimization\")\n",
        "print(\"   - Memory limitations prevent scaling\")\n",
        "print(\"   - Single-threaded processing bottleneck\")\n",
        "\n",
        "# Clean up\n",
        "duckdb_service.close()\n",
        "print(\"\\nCleanup completed successfully\")"
      ],
      "metadata": {
        "id": "performance-analysis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}