{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2ePk7iN7PsjNOGjcTNxCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srnarasim/DataProcessingComparison/blob/main/scenario4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scenario4-title"
      },
      "source": [
        "# Scenario 4: The Machine Learning Feature Pipeline\n",
        "\n",
        "**Constraints**: Complex feature engineering, integration with ML libraries, reproducibility\n",
        "\n",
        "This notebook demonstrates how different data processing tools handle machine learning feature engineering scenarios with:\n",
        "- Complex time-based feature engineering\n",
        "- Integration with ML libraries (scikit-learn, etc.)\n",
        "- Reproducible feature pipelines\n",
        "- Handling of different data types and transformations\n",
        "\n",
        "We'll compare **Pandas**, **Polars**, **DuckDB**, and **Spark** for ML feature engineering workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages for ML feature engineering scenario\n",
        "!pip install polars duckdb pyarrow scikit-learn joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and setup\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import duckdb\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "imports-setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate comprehensive ML dataset\n",
        "def generate_ml_dataset(n_customers=50000, n_transactions_per_customer=20):\n",
        "    \"\"\"Generate realistic dataset for ML feature engineering\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    print(f\"Generating ML dataset: {n_customers:,} customers, ~{n_transactions_per_customer} transactions each\")\n",
        "    \n",
        "    # Customer demographics\n",
        "    customers = []\n",
        "    for i in range(n_customers):\n",
        "        customer = {\n",
        "            'customer_id': f\"CUST_{i:06d}\",\n",
        "            'age': np.random.randint(18, 80),\n",
        "            'gender': np.random.choice(['M', 'F'], p=[0.48, 0.52]),\n",
        "            'income_bracket': np.random.choice(['Low', 'Medium', 'High'], p=[0.3, 0.5, 0.2]),\n",
        "            'registration_date': datetime(2020, 1, 1) + timedelta(days=np.random.randint(0, 1460)),\n",
        "            'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia'], p=[0.2, 0.15, 0.1, 0.1, 0.1, 0.35]),\n",
        "            'preferred_channel': np.random.choice(['Online', 'Mobile', 'Store'], p=[0.4, 0.4, 0.2])\n",
        "        }\n",
        "        customers.append(customer)\n",
        "    \n",
        "    customers_df = pd.DataFrame(customers)\n",
        "    \n",
        "    # Generate transactions with realistic patterns\n",
        "    transactions = []\n",
        "    products = [f\"PROD_{i:04d}\" for i in range(1, 1001)]\n",
        "    categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty', 'Food', 'Automotive']\n",
        "    \n",
        "    for _, customer in customers_df.iterrows():\n",
        "        # Number of transactions varies by customer characteristics\n",
        "        base_transactions = n_transactions_per_customer\n",
        "        if customer['income_bracket'] == 'High':\n",
        "            base_transactions = int(base_transactions * 1.5)\n",
        "        elif customer['income_bracket'] == 'Low':\n",
        "            base_transactions = int(base_transactions * 0.7)\n",
        "        \n",
        "        n_txns = max(1, np.random.poisson(base_transactions))\n",
        "        \n",
        "        for txn_idx in range(n_txns):\n",
        "            # Transaction timing patterns\n",
        "            days_since_reg = (datetime.now() - customer['registration_date']).days\n",
        "            txn_date = customer['registration_date'] + timedelta(\n",
        "                days=np.random.randint(0, min(days_since_reg, 1460))\n",
        "            )\n",
        "            \n",
        "            # Amount varies by income and category\n",
        "            base_amount = np.random.lognormal(3, 1)\n",
        "            if customer['income_bracket'] == 'High':\n",
        "                base_amount *= 2\n",
        "            elif customer['income_bracket'] == 'Low':\n",
        "                base_amount *= 0.6\n",
        "            \n",
        "            category = np.random.choice(categories)\n",
        "            if category == 'Electronics':\n",
        "                base_amount *= 1.5\n",
        "            elif category == 'Food':\n",
        "                base_amount *= 0.3\n",
        "            \n",
        "            transaction = {\n",
        "                'transaction_id': f\"TXN_{len(transactions):08d}\",\n",
        "                'customer_id': customer['customer_id'],\n",
        "                'product_id': np.random.choice(products),\n",
        "                'product_category': category,\n",
        "                'amount': round(base_amount, 2),\n",
        "                'transaction_date': txn_date,\n",
        "                'channel': np.random.choice(['Online', 'Mobile', 'Store'], \n",
        "                                          p=[0.5, 0.3, 0.2] if customer['preferred_channel'] == 'Online' else\n",
        "                                            [0.2, 0.6, 0.2] if customer['preferred_channel'] == 'Mobile' else\n",
        "                                            [0.2, 0.2, 0.6]),\n",
        "                'discount_applied': np.random.choice([True, False], p=[0.3, 0.7]),\n",
        "                'payment_method': np.random.choice(['Credit', 'Debit', 'Cash', 'Digital'], p=[0.4, 0.3, 0.1, 0.2])\n",
        "            }\n",
        "            transactions.append(transaction)\n",
        "    \n",
        "    transactions_df = pd.DataFrame(transactions)\n",
        "    \n",
        "    # Create target variable: customer churn (will churn in next 90 days)\n",
        "    # Based on recency, frequency, monetary patterns\n",
        "    customer_metrics = (\n",
        "        transactions_df.groupby('customer_id')\n",
        "        .agg({\n",
        "            'transaction_date': ['min', 'max', 'count'],\n",
        "            'amount': ['sum', 'mean']\n",
        "        })\n",
        "        .round(2)\n",
        "    )\n",
        "    \n",
        "    customer_metrics.columns = ['first_purchase', 'last_purchase', 'frequency', 'monetary', 'avg_amount']\n",
        "    customer_metrics = customer_metrics.reset_index()\n",
        "    \n",
        "    # Calculate recency (days since last purchase)\n",
        "    reference_date = datetime.now()\n",
        "    customer_metrics['recency'] = (reference_date - customer_metrics['last_purchase']).dt.days\n",
        "    customer_metrics['customer_lifetime'] = (customer_metrics['last_purchase'] - customer_metrics['first_purchase']).dt.days\n",
        "    \n",
        "    # Create churn target based on RFM patterns\n",
        "    def calculate_churn_probability(row):\n",
        "        score = 0\n",
        "        # High recency increases churn probability\n",
        "        if row['recency'] > 180: score += 0.4\n",
        "        elif row['recency'] > 90: score += 0.2\n",
        "        \n",
        "        # Low frequency increases churn probability\n",
        "        if row['frequency'] < 5: score += 0.3\n",
        "        elif row['frequency'] < 10: score += 0.1\n",
        "        \n",
        "        # Low monetary value increases churn probability\n",
        "        if row['monetary'] < 100: score += 0.2\n",
        "        elif row['monetary'] < 500: score += 0.1\n",
        "        \n",
        "        return min(score, 0.8)  # Cap at 80% probability\n",
        "    \n",
        "    customer_metrics['churn_probability'] = customer_metrics.apply(calculate_churn_probability, axis=1)\n",
        "    customer_metrics['will_churn'] = np.random.binomial(1, customer_metrics['churn_probability'])\n",
        "    \n",
        "    # Merge with customer demographics\n",
        "    customers_with_target = customers_df.merge(customer_metrics[['customer_id', 'will_churn']], on='customer_id')\n",
        "    \n",
        "    print(f\"Generated {len(transactions_df):,} transactions for {len(customers_df):,} customers\")\n",
        "    print(f\"Churn rate: {customers_with_target['will_churn'].mean():.1%}\")\n",
        "    \n",
        "    # Save datasets\n",
        "    transactions_df.to_parquet(\"ml_transactions.parquet\")\n",
        "    customers_with_target.to_parquet(\"ml_customers.parquet\")\n",
        "    \n",
        "    return transactions_df, customers_with_target\n",
        "\n",
        "# Generate the ML dataset\n",
        "transactions_df, customers_df = generate_ml_dataset()\n",
        "print(\"\\nSample transaction data:\")\n",
        "print(transactions_df.head())\n",
        "print(\"\\nSample customer data:\")\n",
        "print(customers_df.head())"
      ],
      "metadata": {
        "id": "generate-ml-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas Feature Engineering - Rich Ecosystem Integration\n",
        "\n",
        "Pandas excels at complex feature engineering with its rich ecosystem and flexible APIs, though it can be slow on large datasets."
      ],
      "metadata": {
        "id": "pandas-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_feature_engineering(transactions_df: pd.DataFrame, customers_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Comprehensive feature engineering with pandas\"\"\"\n",
        "    print(\"=== Pandas Feature Engineering ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Ensure datetime types\n",
        "    transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])\n",
        "    customers_df['registration_date'] = pd.to_datetime(customers_df['registration_date'])\n",
        "    \n",
        "    # Sort for time-based features\n",
        "    transactions_df = transactions_df.sort_values(['customer_id', 'transaction_date'])\n",
        "    \n",
        "    print(\"Creating time-based features...\")\n",
        "    \n",
        "    # 1. RFM Features (Recency, Frequency, Monetary)\n",
        "    reference_date = transactions_df['transaction_date'].max()\n",
        "    \n",
        "    rfm_features = (\n",
        "        transactions_df.groupby('customer_id')\n",
        "        .agg({\n",
        "            'transaction_date': ['min', 'max', 'count'],\n",
        "            'amount': ['sum', 'mean', 'std', 'min', 'max']\n",
        "        })\n",
        "        .round(2)\n",
        "    )\n",
        "    \n",
        "    rfm_features.columns = ['first_purchase', 'last_purchase', 'frequency', \n",
        "                           'monetary', 'avg_amount', 'std_amount', 'min_amount', 'max_amount']\n",
        "    rfm_features = rfm_features.reset_index()\n",
        "    \n",
        "    # Calculate recency and customer lifetime\n",
        "    rfm_features['recency'] = (reference_date - rfm_features['last_purchase']).dt.days\n",
        "    rfm_features['customer_lifetime'] = (rfm_features['last_purchase'] - rfm_features['first_purchase']).dt.days\n",
        "    rfm_features['purchase_frequency'] = rfm_features['frequency'] / (rfm_features['customer_lifetime'] + 1)\n",
        "    \n",
        "    print(\"Creating rolling window features...\")\n",
        "    \n",
        "    # 2. Rolling Window Features (last 30, 60, 90 days)\n",
        "    rolling_features_list = []\n",
        "    \n",
        "    for customer_id in transactions_df['customer_id'].unique():\n",
        "        customer_txns = transactions_df[transactions_df['customer_id'] == customer_id].copy()\n",
        "        customer_txns = customer_txns.set_index('transaction_date').sort_index()\n",
        "        \n",
        "        # Rolling aggregations\n",
        "        rolling_30d = customer_txns['amount'].rolling('30D').agg(['sum', 'count', 'mean']).fillna(0)\n",
        "        rolling_60d = customer_txns['amount'].rolling('60D').agg(['sum', 'count', 'mean']).fillna(0)\n",
        "        rolling_90d = customer_txns['amount'].rolling('90D').agg(['sum', 'count', 'mean']).fillna(0)\n",
        "        \n",
        "        # Take the last values (most recent)\n",
        "        if len(rolling_30d) > 0:\n",
        "            rolling_features_list.append({\n",
        "                'customer_id': customer_id,\n",
        "                'rolling_30d_sum': rolling_30d['sum'].iloc[-1],\n",
        "                'rolling_30d_count': rolling_30d['count'].iloc[-1],\n",
        "                'rolling_30d_mean': rolling_30d['mean'].iloc[-1],\n",
        "                'rolling_60d_sum': rolling_60d['sum'].iloc[-1],\n",
        "                'rolling_60d_count': rolling_60d['count'].iloc[-1],\n",
        "                'rolling_60d_mean': rolling_60d['mean'].iloc[-1],\n",
        "                'rolling_90d_sum': rolling_90d['sum'].iloc[-1],\n",
        "                'rolling_90d_count': rolling_90d['count'].iloc[-1],\n",
        "                'rolling_90d_mean': rolling_90d['mean'].iloc[-1]\n",
        "            })\n",
        "    \n",
        "    rolling_features = pd.DataFrame(rolling_features_list).fillna(0)\n",
        "    \n",
        "    print(\"Creating behavioral features...\")\n",
        "    \n",
        "    # 3. Behavioral Features\n",
        "    behavioral_features = (\n",
        "        transactions_df.groupby('customer_id')\n",
        "        .agg({\n",
        "            'product_category': lambda x: x.nunique(),  # Category diversity\n",
        "            'channel': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown',  # Preferred channel\n",
        "            'discount_applied': 'mean',  # Discount usage rate\n",
        "            'payment_method': lambda x: x.nunique()  # Payment method diversity\n",
        "        })\n",
        "        .rename(columns={\n",
        "            'product_category': 'category_diversity',\n",
        "            'channel': 'preferred_channel',\n",
        "            'discount_applied': 'discount_usage_rate',\n",
        "            'payment_method': 'payment_method_diversity'\n",
        "        })\n",
        "        .reset_index()\n",
        "    )\n",
        "    \n",
        "    # 4. Seasonal and Temporal Features\n",
        "    transactions_df['hour'] = transactions_df['transaction_date'].dt.hour\n",
        "    transactions_df['day_of_week'] = transactions_df['transaction_date'].dt.dayofweek\n",
        "    transactions_df['month'] = transactions_df['transaction_date'].dt.month\n",
        "    transactions_df['is_weekend'] = transactions_df['day_of_week'].isin([5, 6])\n",
        "    \n",
        "    temporal_features = (\n",
        "        transactions_df.groupby('customer_id')\n",
        "        .agg({\n",
        "            'hour': 'mean',  # Average transaction hour\n",
        "            'is_weekend': 'mean',  # Weekend transaction rate\n",
        "            'month': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 6  # Preferred month\n",
        "        })\n",
        "        .rename(columns={\n",
        "            'hour': 'avg_transaction_hour',\n",
        "            'is_weekend': 'weekend_transaction_rate',\n",
        "            'month': 'preferred_month'\n",
        "        })\n",
        "        .reset_index()\n",
        "    )\n",
        "    \n",
        "    print(\"Merging all features...\")\n",
        "    \n",
        "    # Merge all features\n",
        "    features = customers_df.copy()\n",
        "    features = features.merge(rfm_features, on='customer_id', how='left')\n",
        "    features = features.merge(rolling_features, on='customer_id', how='left')\n",
        "    features = features.merge(behavioral_features, on='customer_id', how='left')\n",
        "    features = features.merge(temporal_features, on='customer_id', how='left')\n",
        "    \n",
        "    # Fill missing values\n",
        "    numeric_columns = features.select_dtypes(include=[np.number]).columns\n",
        "    features[numeric_columns] = features[numeric_columns].fillna(0)\n",
        "    \n",
        "    # Create derived features\n",
        "    features['customer_age_days'] = (reference_date - features['registration_date']).dt.days\n",
        "    features['avg_days_between_purchases'] = features['customer_lifetime'] / (features['frequency'] + 1)\n",
        "    features['monetary_per_frequency'] = features['monetary'] / (features['frequency'] + 1)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"Pandas feature engineering completed in {processing_time:.2f} seconds\")\n",
        "    print(f\"Created {len(features.columns)} features for {len(features)} customers\")\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Run pandas feature engineering\n",
        "pandas_features = pandas_feature_engineering(transactions_df.copy(), customers_df.copy())\n",
        "print(f\"\\nPandas features shape: {pandas_features.shape}\")\n",
        "print(\"\\nFeature columns:\")\n",
        "print(pandas_features.columns.tolist())"
      ],
      "metadata": {
        "id": "pandas-features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polars Feature Engineering - High Performance with Modern APIs\n",
        "\n",
        "Polars offers excellent performance for feature engineering but may require conversion to pandas for some ML libraries."
      ],
      "metadata": {
        "id": "polars-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polars_feature_engineering(transactions_df: pd.DataFrame, customers_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"High-performance feature engineering with Polars\"\"\"\n",
        "    print(\"=== Polars Feature Engineering ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Convert to Polars DataFrames\n",
        "    transactions_pl = pl.from_pandas(transactions_df)\n",
        "    customers_pl = pl.from_pandas(customers_df)\n",
        "    \n",
        "    # Ensure datetime types\n",
        "    transactions_pl = transactions_pl.with_columns([\n",
        "        pl.col(\"transaction_date\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False)\n",
        "    ])\n",
        "    \n",
        "    customers_pl = customers_pl.with_columns([\n",
        "        pl.col(\"registration_date\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False)\n",
        "    ])\n",
        "    \n",
        "    print(\"Creating RFM and basic features...\")\n",
        "    \n",
        "    # Get reference date\n",
        "    reference_date = transactions_pl.select(pl.col(\"transaction_date\").max()).item()\n",
        "    \n",
        "    # 1. RFM Features with efficient aggregations\n",
        "    rfm_features = (\n",
        "        transactions_pl\n",
        "        .group_by(\"customer_id\")\n",
        "        .agg([\n",
        "            pl.col(\"transaction_date\").min().alias(\"first_purchase\"),\n",
        "            pl.col(\"transaction_date\").max().alias(\"last_purchase\"),\n",
        "            pl.col(\"transaction_id\").count().alias(\"frequency\"),\n",
        "            pl.col(\"amount\").sum().alias(\"monetary\"),\n",
        "            pl.col(\"amount\").mean().alias(\"avg_amount\"),\n",
        "            pl.col(\"amount\").std().alias(\"std_amount\"),\n",
        "            pl.col(\"amount\").min().alias(\"min_amount\"),\n",
        "            pl.col(\"amount\").max().alias(\"max_amount\")\n",
        "        ])\n",
        "        .with_columns([\n",
        "            (pl.lit(reference_date) - pl.col(\"last_purchase\")).dt.total_days().alias(\"recency\"),\n",
        "            (pl.col(\"last_purchase\") - pl.col(\"first_purchase\")).dt.total_days().alias(\"customer_lifetime\")\n",
        "        ])\n",
        "        .with_columns([\n",
        "            (pl.col(\"frequency\") / (pl.col(\"customer_lifetime\") + 1)).alias(\"purchase_frequency\")\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    print(\"Creating rolling window features...\")\n",
        "    \n",
        "    # 2. Rolling Window Features (more efficient with Polars)\n",
        "    rolling_features = (\n",
        "        transactions_pl\n",
        "        .sort([\"customer_id\", \"transaction_date\"])\n",
        "        .with_columns([\n",
        "            # 30-day rolling features\n",
        "            pl.col(\"amount\")\n",
        "            .rolling_sum(window_size=\"30d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_30d_sum\"),\n",
        "            \n",
        "            pl.col(\"amount\")\n",
        "            .rolling_mean(window_size=\"30d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_30d_mean\"),\n",
        "            \n",
        "            # 60-day rolling features\n",
        "            pl.col(\"amount\")\n",
        "            .rolling_sum(window_size=\"60d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_60d_sum\"),\n",
        "            \n",
        "            pl.col(\"amount\")\n",
        "            .rolling_mean(window_size=\"60d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_60d_mean\"),\n",
        "            \n",
        "            # 90-day rolling features\n",
        "            pl.col(\"amount\")\n",
        "            .rolling_sum(window_size=\"90d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_90d_sum\"),\n",
        "            \n",
        "            pl.col(\"amount\")\n",
        "            .rolling_mean(window_size=\"90d\", by=\"transaction_date\")\n",
        "            .over(\"customer_id\")\n",
        "            .alias(\"rolling_90d_mean\")\n",
        "        ])\n",
        "        .group_by(\"customer_id\")\n",
        "        .agg([\n",
        "            pl.col(\"rolling_30d_sum\").last(),\n",
        "            pl.col(\"rolling_30d_mean\").last(),\n",
        "            pl.col(\"rolling_60d_sum\").last(),\n",
        "            pl.col(\"rolling_60d_mean\").last(),\n",
        "            pl.col(\"rolling_90d_sum\").last(),\n",
        "            pl.col(\"rolling_90d_mean\").last()\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    print(\"Creating behavioral and temporal features...\")\n",
        "    \n",
        "    # 3. Behavioral Features\n",
        "    behavioral_features = (\n",
        "        transactions_pl\n",
        "        .group_by(\"customer_id\")\n",
        "        .agg([\n",
        "            pl.col(\"product_category\").n_unique().alias(\"category_diversity\"),\n",
        "            pl.col(\"channel\").mode().first().alias(\"preferred_channel\"),\n",
        "            pl.col(\"discount_applied\").mean().alias(\"discount_usage_rate\"),\n",
        "            pl.col(\"payment_method\").n_unique().alias(\"payment_method_diversity\")\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    # 4. Temporal Features\n",
        "    temporal_features = (\n",
        "        transactions_pl\n",
        "        .with_columns([\n",
        "            pl.col(\"transaction_date\").dt.hour().alias(\"hour\"),\n",
        "            pl.col(\"transaction_date\").dt.weekday().alias(\"day_of_week\"),\n",
        "            pl.col(\"transaction_date\").dt.month().alias(\"month\"),\n",
        "            pl.col(\"transaction_date\").dt.weekday().is_in([5, 6]).alias(\"is_weekend\")\n",
        "        ])\n",
        "        .group_by(\"customer_id\")\n",
        "        .agg([\n",
        "            pl.col(\"hour\").mean().alias(\"avg_transaction_hour\"),\n",
        "            pl.col(\"is_weekend\").mean().alias(\"weekend_transaction_rate\"),\n",
        "            pl.col(\"month\").mode().first().alias(\"preferred_month\")\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    print(\"Merging all features...\")\n",
        "    \n",
        "    # Merge all features\n",
        "    features = (\n",
        "        customers_pl\n",
        "        .join(rfm_features, on=\"customer_id\", how=\"left\")\n",
        "        .join(rolling_features, on=\"customer_id\", how=\"left\")\n",
        "        .join(behavioral_features, on=\"customer_id\", how=\"left\")\n",
        "        .join(temporal_features, on=\"customer_id\", how=\"left\")\n",
        "    )\n",
        "    \n",
        "    # Create derived features\n",
        "    features = features.with_columns([\n",
        "        (pl.lit(reference_date) - pl.col(\"registration_date\")).dt.total_days().alias(\"customer_age_days\"),\n",
        "        (pl.col(\"customer_lifetime\") / (pl.col(\"frequency\") + 1)).alias(\"avg_days_between_purchases\"),\n",
        "        (pl.col(\"monetary\") / (pl.col(\"frequency\") + 1)).alias(\"monetary_per_frequency\")\n",
        "    ])\n",
        "    \n",
        "    # Fill null values\n",
        "    features = features.fill_null(0)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"Polars feature engineering completed in {processing_time:.2f} seconds\")\n",
        "    \n",
        "    # Convert back to pandas for ML compatibility\n",
        "    features_pd = features.to_pandas()\n",
        "    print(f\"Created {len(features_pd.columns)} features for {len(features_pd)} customers\")\n",
        "    \n",
        "    return features_pd\n",
        "\n",
        "# Run Polars feature engineering\n",
        "polars_features = polars_feature_engineering(transactions_df.copy(), customers_df.copy())\n",
        "print(f\"\\nPolars features shape: {polars_features.shape}\")"
      ],
      "metadata": {
        "id": "polars-features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DuckDB Feature Engineering - SQL-First Complex Features\n",
        "\n",
        "DuckDB excels at complex SQL-based feature engineering that would be difficult to express in DataFrame APIs."
      ],
      "metadata": {
        "id": "duckdb-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duckdb_feature_engineering(transactions_df: pd.DataFrame, customers_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"SQL-first feature engineering with DuckDB\"\"\"\n",
        "    print(\"=== DuckDB Feature Engineering ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    conn = duckdb.connect(\":memory:\")\n",
        "    \n",
        "    try:\n",
        "        # Load data into DuckDB\n",
        "        conn.register(\"transactions\", transactions_df)\n",
        "        conn.register(\"customers\", customers_df)\n",
        "        \n",
        "        print(\"Creating comprehensive features with SQL...\")\n",
        "        \n",
        "        # Complex feature engineering with SQL\n",
        "        features_query = \"\"\"\n",
        "        WITH transaction_features AS (\n",
        "            SELECT \n",
        "                customer_id,\n",
        "                -- RFM Features\n",
        "                DATE_DIFF('day', MAX(transaction_date), CURRENT_DATE) as recency,\n",
        "                COUNT(*) as frequency,\n",
        "                SUM(amount) as monetary,\n",
        "                AVG(amount) as avg_amount,\n",
        "                STDDEV(amount) as std_amount,\n",
        "                MIN(amount) as min_amount,\n",
        "                MAX(amount) as max_amount,\n",
        "                \n",
        "                -- Customer Lifetime\n",
        "                DATE_DIFF('day', MIN(transaction_date), MAX(transaction_date)) as customer_lifetime,\n",
        "                \n",
        "                -- Sequential Features\n",
        "                AVG(amount - LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY transaction_date)) as avg_amount_change,\n",
        "                \n",
        "                -- Percentile-based features\n",
        "                PERCENT_RANK() OVER (ORDER BY SUM(amount)) as monetary_percentile,\n",
        "                PERCENT_RANK() OVER (ORDER BY COUNT(*)) as frequency_percentile,\n",
        "                \n",
        "                -- Behavioral Features\n",
        "                COUNT(DISTINCT product_category) as category_diversity,\n",
        "                COUNT(DISTINCT payment_method) as payment_method_diversity,\n",
        "                AVG(CASE WHEN discount_applied THEN 1.0 ELSE 0.0 END) as discount_usage_rate,\n",
        "                \n",
        "                -- Temporal Features\n",
        "                AVG(EXTRACT(HOUR FROM transaction_date)) as avg_transaction_hour,\n",
        "                AVG(CASE WHEN EXTRACT(DOW FROM transaction_date) IN (0, 6) THEN 1.0 ELSE 0.0 END) as weekend_transaction_rate,\n",
        "                MODE() WITHIN GROUP (ORDER BY EXTRACT(MONTH FROM transaction_date)) as preferred_month,\n",
        "                MODE() WITHIN GROUP (ORDER BY channel) as preferred_channel,\n",
        "                \n",
        "                -- Rolling Window Features (last 30, 60, 90 days)\n",
        "                SUM(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 30 DAY THEN amount ELSE 0 END) as rolling_30d_sum,\n",
        "                COUNT(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 30 DAY THEN 1 END) as rolling_30d_count,\n",
        "                AVG(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 30 DAY THEN amount END) as rolling_30d_mean,\n",
        "                \n",
        "                SUM(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 60 DAY THEN amount ELSE 0 END) as rolling_60d_sum,\n",
        "                COUNT(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 60 DAY THEN 1 END) as rolling_60d_count,\n",
        "                AVG(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 60 DAY THEN amount END) as rolling_60d_mean,\n",
        "                \n",
        "                SUM(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 90 DAY THEN amount ELSE 0 END) as rolling_90d_sum,\n",
        "                COUNT(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 90 DAY THEN 1 END) as rolling_90d_count,\n",
        "                AVG(CASE WHEN transaction_date >= CURRENT_DATE - INTERVAL 90 DAY THEN amount END) as rolling_90d_mean,\n",
        "                \n",
        "                -- Advanced Features\n",
        "                VARIANCE(amount) as amount_variance,\n",
        "                (MAX(amount) - MIN(amount)) / NULLIF(AVG(amount), 0) as amount_range_ratio,\n",
        "                \n",
        "                -- Trend Features\n",
        "                CORR(EXTRACT(EPOCH FROM transaction_date), amount) as amount_time_correlation,\n",
        "                \n",
        "                -- Category Preferences\n",
        "                MODE() WITHIN GROUP (ORDER BY product_category) as preferred_category,\n",
        "                MAX(amount) FILTER (WHERE product_category = 'Electronics') as max_electronics_spend,\n",
        "                COUNT(*) FILTER (WHERE product_category = 'Electronics') as electronics_frequency\n",
        "                \n",
        "            FROM transactions\n",
        "            GROUP BY customer_id\n",
        "        ),\n",
        "        \n",
        "        customer_features AS (\n",
        "            SELECT \n",
        "                c.*,\n",
        "                DATE_DIFF('day', registration_date, CURRENT_DATE) as customer_age_days,\n",
        "                CASE \n",
        "                    WHEN age < 25 THEN 'Young'\n",
        "                    WHEN age < 45 THEN 'Middle'\n",
        "                    ELSE 'Senior'\n",
        "                END as age_group\n",
        "            FROM customers c\n",
        "        )\n",
        "        \n",
        "        SELECT \n",
        "            cf.*,\n",
        "            tf.* EXCLUDE (customer_id),\n",
        "            \n",
        "            -- Derived Features\n",
        "            tf.frequency::FLOAT / NULLIF(tf.customer_lifetime + 1, 0) as purchase_frequency,\n",
        "            tf.monetary::FLOAT / NULLIF(tf.frequency, 0) as monetary_per_frequency,\n",
        "            tf.customer_lifetime::FLOAT / NULLIF(tf.frequency, 0) as avg_days_between_purchases,\n",
        "            \n",
        "            -- Interaction Features\n",
        "            CASE WHEN cf.income_bracket = 'High' AND tf.monetary > 1000 THEN 1 ELSE 0 END as high_value_customer,\n",
        "            CASE WHEN tf.recency > 90 AND tf.frequency < 5 THEN 1 ELSE 0 END as at_risk_customer\n",
        "            \n",
        "        FROM customer_features cf\n",
        "        LEFT JOIN transaction_features tf ON cf.customer_id = tf.customer_id\n",
        "        \"\"\"\n",
        "        \n",
        "        # Execute the complex query\n",
        "        features = conn.execute(features_query).df()\n",
        "        \n",
        "        # Handle null values\n",
        "        numeric_columns = features.select_dtypes(include=[np.number]).columns\n",
        "        features[numeric_columns] = features[numeric_columns].fillna(0)\n",
        "        \n",
        "        # Fill categorical nulls\n",
        "        categorical_columns = features.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            if col not in ['customer_id']:\n",
        "                features[col] = features[col].fillna('Unknown')\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        print(f\"DuckDB feature engineering completed in {processing_time:.2f} seconds\")\n",
        "        print(f\"Created {len(features.columns)} features for {len(features)} customers\")\n",
        "        \n",
        "        return features\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"DuckDB feature engineering failed: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# Run DuckDB feature engineering\n",
        "duckdb_features = duckdb_feature_engineering(transactions_df.copy(), customers_df.copy())\n",
        "print(f\"\\nDuckDB features shape: {duckdb_features.shape}\")\n",
        "print(\"\\nUnique DuckDB features (not in pandas):\")\n",
        "duckdb_only_features = set(duckdb_features.columns) - set(pandas_features.columns)\n",
        "print(list(duckdb_only_features)[:10])  # Show first 10"
      ],
      "metadata": {
        "id": "duckdb-features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model Training and Evaluation\n",
        "\n",
        "Let's train ML models using features from each tool to compare their effectiveness and integration with the ML ecosystem."
      ],
      "metadata": {
        "id": "ml-training-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_ml_data(features_df: pd.DataFrame, target_col: str = 'will_churn') -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    \"\"\"Prepare features for ML training\"\"\"\n",
        "    # Separate features and target\n",
        "    feature_columns = [col for col in features_df.columns \n",
        "                      if col not in ['customer_id', target_col, 'registration_date']]\n",
        "    \n",
        "    X = features_df[feature_columns].copy()\n",
        "    y = features_df[target_col].copy()\n",
        "    \n",
        "    # Handle categorical variables\n",
        "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    # Simple label encoding for categorical variables\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "    \n",
        "    # Handle any remaining non-numeric columns\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "    \n",
        "    # Fill any remaining NaN values\n",
        "    X = X.fillna(0)\n",
        "    \n",
        "    return X.values, y.values, X.columns.tolist()\n",
        "\n",
        "def train_and_evaluate_model(X: np.ndarray, y: np.ndarray, feature_names: List[str], model_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Train and evaluate a machine learning model\"\"\"\n",
        "    print(f\"\\n=== Training {model_name} Model ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
        "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "    print(f\"Churn rate - Train: {y_train.mean():.1%}, Test: {y_test.mean():.1%}\")\n",
        "    \n",
        "    # Create preprocessing pipeline\n",
        "    preprocessor = StandardScaler()\n",
        "    \n",
        "    # Create and train model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Train model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "    print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feature_importance_df.head(10))\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'auc_score': auc_score,\n",
        "        'training_time': training_time,\n",
        "        'n_features': X.shape[1],\n",
        "        'feature_importance': feature_importance_df,\n",
        "        'pipeline': pipeline\n",
        "    }\n",
        "\n",
        "# Train models with features from each tool\n",
        "ml_results = []\n",
        "\n",
        "# Pandas features\n",
        "try:\n",
        "    X_pandas, y_pandas, features_pandas = prepare_ml_data(pandas_features)\n",
        "    pandas_result = train_and_evaluate_model(X_pandas, y_pandas, features_pandas, \"Pandas Features\")\n",
        "    ml_results.append(pandas_result)\n",
        "except Exception as e:\n",
        "    print(f\"Pandas ML training failed: {e}\")\n",
        "\n",
        "# Polars features\n",
        "try:\n",
        "    X_polars, y_polars, features_polars = prepare_ml_data(polars_features)\n",
        "    polars_result = train_and_evaluate_model(X_polars, y_polars, features_polars, \"Polars Features\")\n",
        "    ml_results.append(polars_result)\n",
        "except Exception as e:\n",
        "    print(f\"Polars ML training failed: {e}\")\n",
        "\n",
        "# DuckDB features\n",
        "try:\n",
        "    X_duckdb, y_duckdb, features_duckdb = prepare_ml_data(duckdb_features)\n",
        "    duckdb_result = train_and_evaluate_model(X_duckdb, y_duckdb, features_duckdb, \"DuckDB Features\")\n",
        "    ml_results.append(duckdb_result)\n",
        "except Exception as e:\n",
        "    print(f\"DuckDB ML training failed: {e}\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "if ml_results:\n",
        "    comparison_data = []\n",
        "    for result in ml_results:\n",
        "        comparison_data.append({\n",
        "            'Tool': result['model_name'],\n",
        "            'AUC Score': result['auc_score'],\n",
        "            'Training Time (s)': result['training_time'],\n",
        "            'Number of Features': result['n_features']\n",
        "        })\n",
        "    \n",
        "    ml_comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n=== ML Model Performance Comparison ===\")\n",
        "    print(ml_comparison_df.round(4))\n",
        "else:\n",
        "    print(\"No ML results to compare\")"
      ],
      "metadata": {
        "id": "ml-training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Analysis and Visualization\n",
        "\n",
        "Let's analyze the performance and capabilities of each tool for ML feature engineering."
      ],
      "metadata": {
        "id": "analysis-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Performance visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "if ml_results:\n",
        "    # 1. Model Performance Comparison\n",
        "    tools = [r['model_name'].replace(' Features', '') for r in ml_results]\n",
        "    auc_scores = [r['auc_score'] for r in ml_results]\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(tools)]\n",
        "    \n",
        "    bars1 = ax1.bar(tools, auc_scores, color=colors)\n",
        "    ax1.set_title('ML Model Performance (AUC Score)')\n",
        "    ax1.set_ylabel('AUC Score')\n",
        "    ax1.set_ylim(0.5, 1.0)\n",
        "    \n",
        "    for bar, score in zip(bars1, auc_scores):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 2. Feature Engineering Time\n",
        "    # Note: We'd need to track feature engineering times separately\n",
        "    # For now, we'll use training times as a proxy\n",
        "    training_times = [r['training_time'] for r in ml_results]\n",
        "    bars2 = ax2.bar(tools, training_times, color=colors)\n",
        "    ax2.set_title('Training Time')\n",
        "    ax2.set_ylabel('Time (seconds)')\n",
        "    \n",
        "    for bar, time in zip(bars2, training_times):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(training_times)*0.02, \n",
        "                f'{time:.1f}s', ha='center', va='bottom')\n",
        "    \n",
        "    # 3. Number of Features Created\n",
        "    n_features = [r['n_features'] for r in ml_results]\n",
        "    bars3 = ax3.bar(tools, n_features, color=colors)\n",
        "    ax3.set_title('Number of Features Created')\n",
        "    ax3.set_ylabel('Feature Count')\n",
        "    \n",
        "    for bar, count in zip(bars3, n_features):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(n_features)*0.02, \n",
        "                f'{count}', ha='center', va='bottom')\n",
        "\n",
        "# 4. ML Feature Engineering Capability Matrix\n",
        "capability_data = {\n",
        "    'Tool': ['Pandas', 'Polars', 'DuckDB', 'Spark'],\n",
        "    'Feature Engineering Speed': [2, 5, 4, 4],\n",
        "    'ML Ecosystem Integration': [5, 3, 4, 4],\n",
        "    'Complex Transformations': [4, 3, 5, 4],\n",
        "    'Time Series Features': [5, 4, 4, 3],\n",
        "    'Memory Efficiency': [2, 4, 4, 5],\n",
        "    'Reproducibility': [4, 4, 5, 5]\n",
        "}\n",
        "\n",
        "capability_df = pd.DataFrame(capability_data)\n",
        "capability_matrix = capability_df.set_index('Tool').T\n",
        "\n",
        "im = ax4.imshow(capability_matrix.values, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
        "ax4.set_xticks(range(len(capability_matrix.columns)))\n",
        "ax4.set_xticklabels(capability_matrix.columns)\n",
        "ax4.set_yticks(range(len(capability_matrix.index)))\n",
        "ax4.set_yticklabels(capability_matrix.index)\n",
        "ax4.set_title('ML Feature Engineering Capabilities')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(capability_matrix.index)):\n",
        "    for j in range(len(capability_matrix.columns)):\n",
        "        ax4.text(j, i, capability_matrix.iloc[i, j], ha='center', va='center', \n",
        "                color='white' if capability_matrix.iloc[i, j] < 3 else 'black')\n",
        "\n",
        "plt.colorbar(im, ax=ax4, label='Capability Score (1-5)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance comparison\n",
        "if len(ml_results) >= 2:\n",
        "    fig, axes = plt.subplots(1, len(ml_results), figsize=(5*len(ml_results), 6))\n",
        "    if len(ml_results) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, result in enumerate(ml_results):\n",
        "        top_features = result['feature_importance'].head(10)\n",
        "        axes[i].barh(range(len(top_features)), top_features['importance'])\n",
        "        axes[i].set_yticks(range(len(top_features)))\n",
        "        axes[i].set_yticklabels(top_features['feature'], fontsize=8)\n",
        "        axes[i].set_title(f'{result[\"model_name\"]}\\nTop Features')\n",
        "        axes[i].set_xlabel('Feature Importance')\n",
        "        axes[i].invert_yaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n=== SCENARIO 4 CONCLUSIONS ===\")\n",
        "print(\"\\n🏆 WINNER: Pandas for Rapid ML Prototyping\")\n",
        "print(\"   - Unmatched ecosystem integration with scikit-learn, XGBoost, etc.\")\n",
        "print(\"   - Flexible APIs for complex feature transformations\")\n",
        "print(\"   - Extensive time series functionality\")\n",
        "print(\"   - Rich visualization and debugging capabilities\")\n",
        "print(\"   - Seamless integration with Jupyter notebooks\")\n",
        "\n",
        "print(\"\\n🥈 RUNNER-UP: DuckDB for Complex SQL-based Features\")\n",
        "print(\"   - Excellent for complex analytical feature engineering\")\n",
        "print(\"   - SQL expressiveness for window functions and aggregations\")\n",
        "print(\"   - Easy integration with pandas for ML workflows\")\n",
        "print(\"   - Great for features that are hard to express in DataFrame APIs\")\n",
        "\n",
        "print(\"\\n🚀 Polars: High-Performance Feature Computation\")\n",
        "print(\"   - Excellent performance for large-scale feature engineering\")\n",
        "print(\"   - Memory-efficient processing\")\n",
        "print(\"   - Often requires conversion to pandas for ML ecosystem\")\n",
        "print(\"   - Growing ML integration but still limited\")\n",
        "\n",
        "print(\"\\n⚡ Spark: Production ML Pipelines at Scale\")\n",
        "print(\"   - Best for large-scale production ML feature pipelines\")\n",
        "print(\"   - MLlib integration for distributed ML\")\n",
        "print(\"   - Built-in feature transformers and pipelines\")\n",
        "print(\"   - Overkill for exploratory feature engineering\")\n",
        "\n",
        "print(\"\\n📊 Key Insights:\")\n",
        "print(\"   - Pandas remains king for ML prototyping and experimentation\")\n",
        "print(\"   - DuckDB excels at creating complex analytical features\")\n",
        "print(\"   - Polars is great for performance but needs pandas bridge\")\n",
        "print(\"   - Spark is essential for production ML at enterprise scale\")\n",
        "print(\"   - Hybrid approaches often work best in practice\")"
      ],
      "metadata": {
        "id": "performance-analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of all scenarios\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE DATA PROCESSING TOOL COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n📊 SCENARIO WINNERS:\")\n",
        "print(\"   1. Jupyter Notebook Data Scientist → DuckDB/Polars\")\n",
        "print(\"   2. Production ETL Pipeline → Spark\")\n",
        "print(\"   3. Real-Time Analytics Dashboard → DuckDB\")\n",
        "print(\"   4. ML Feature Pipeline → Pandas\")\n",
        "\n",
        "print(\"\\n🎯 TOOL RECOMMENDATIONS BY USE CASE:\")\n",
        "print(\"\\n   🐼 Choose PANDAS when:\")\n",
        "print(\"      • Rapid prototyping and experimentation\")\n",
        "print(\"      • Complex ML feature engineering\")\n",
        "print(\"      • Rich ecosystem integration needed\")\n",
        "print(\"      • Data fits in memory (<5GB)\")\n",
        "\n",
        "print(\"\\n   🐻‍❄️ Choose POLARS when:\")\n",
        "print(\"      • Performance is critical\")\n",
        "print(\"      • Memory efficiency matters\")\n",
        "print(\"      • Data processing workloads (5-100GB)\")\n",
        "print(\"      • Team can invest in learning new APIs\")\n",
        "\n",
        "print(\"\\n   🦆 Choose DUCKDB when:\")\n",
        "print(\"      • Analytical queries and dashboards\")\n",
        "print(\"      • SQL-first approach preferred\")\n",
        "print(\"      • Complex aggregations and window functions\")\n",
        "print(\"      • Single-machine analytical workloads\")\n",
        "\n",
        "print(\"\\n   ⚡ Choose SPARK when:\")\n",
        "print(\"      • Production ETL at scale (>100GB)\")\n",
        "print(\"      • Distributed processing required\")\n",
        "print(\"      • Enterprise fault tolerance needed\")\n",
        "print(\"      • Existing big data infrastructure\")\n",
        "\n",
        "print(\"\\n🔄 HYBRID APPROACHES:\")\n",
        "print(\"   • Use Spark for ETL → DuckDB for analytics\")\n",
        "print(\"   • Use Polars for processing → Pandas for ML\")\n",
        "print(\"   • Use DuckDB for features → Pandas for modeling\")\n",
        "print(\"   • Use multiple tools in the same pipeline\")\n",
        "\n",
        "print(\"\\n💡 FINAL ADVICE:\")\n",
        "print(\"   The 'best' tool depends on your constraints, not just data size.\")\n",
        "print(\"   Consider: team skills, infrastructure, performance needs,\")\n",
        "print(\"   ecosystem requirements, and maintenance overhead.\")\n",
        "print(\"   Don't be afraid to use multiple tools together!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "final-summary"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}