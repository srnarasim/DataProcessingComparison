{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2ePk7iN7PsjNOGjcTNxCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srnarasim/DataProcessingComparison/blob/main/scenario2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scenario2-title"
      },
      "source": [
        "# Scenario 2: The Production ETL Pipeline\n",
        "\n",
        "**Constraints**: Reliability, monitoring, error handling, integration with existing infrastructure\n",
        "\n",
        "This notebook demonstrates how different data processing tools handle production ETL scenarios with:\n",
        "- Large daily data volumes (50GB+ simulated with smaller datasets)\n",
        "- Fault tolerance requirements\n",
        "- Complex transformations\n",
        "- Error handling and monitoring\n",
        "\n",
        "We'll compare **Spark**, **Polars**, **Pandas**, and **DuckDB** for production ETL workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages for production ETL scenario\n",
        "!pip install polars duckdb pyarrow pyspark findspark\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/.local/lib/python3.10/site-packages/pyspark\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and setup\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import duckdb\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "imports-setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate realistic production-scale sample data\n",
        "def generate_production_data(n_rows=1_000_000, n_files=5):\n",
        "    \"\"\"Generate multiple files simulating daily transaction batches\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create data directory\n",
        "    Path(\"data/transactions/2025/01\").mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    customers = [f\"CUST_{i:06d}\" for i in range(1, 50001)]  # 50K customers\n",
        "    products = [f\"PROD_{i:04d}\" for i in range(1, 5001)]   # 5K products\n",
        "    categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty']\n",
        "    \n",
        "    files_created = []\n",
        "    \n",
        "    for file_idx in range(n_files):\n",
        "        # Simulate daily batch with some variation\n",
        "        daily_rows = n_rows + np.random.randint(-50000, 50000)\n",
        "        base_date = datetime(2025, 1, 1) + timedelta(days=file_idx)\n",
        "        \n",
        "        data = {\n",
        "            'order_id': [f\"ORD_{file_idx}_{i:08d}\" for i in range(daily_rows)],\n",
        "            'customer_id': np.random.choice(customers, daily_rows),\n",
        "            'product_id': np.random.choice(products, daily_rows),\n",
        "            'product_category': np.random.choice(categories, daily_rows),\n",
        "            'order_total': np.random.lognormal(3, 1, daily_rows).round(2),\n",
        "            'order_date': [\n",
        "                base_date + timedelta(hours=np.random.randint(0, 24),\n",
        "                                    minutes=np.random.randint(0, 60))\n",
        "                for _ in range(daily_rows)\n",
        "            ],\n",
        "            'processing_batch': f\"2025-01-{file_idx+1:02d}\"\n",
        "        }\n",
        "        \n",
        "        # Add some corrupted data to test error handling (5% of records)\n",
        "        corruption_indices = np.random.choice(daily_rows, size=int(daily_rows * 0.05), replace=False)\n",
        "        for idx in corruption_indices:\n",
        "            if np.random.random() < 0.5:\n",
        "                data['order_total'][idx] = None  # Missing values\n",
        "            else:\n",
        "                data['customer_id'][idx] = \"INVALID_CUSTOMER\"  # Invalid customer\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        filename = f\"data/transactions/2025/01/transactions_2025-01-{file_idx+1:02d}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        files_created.append(filename)\n",
        "        \n",
        "        print(f\"Created {filename} with {daily_rows:,} rows\")\n",
        "    \n",
        "    return files_created\n",
        "\n",
        "# Generate sample data files\n",
        "data_files = generate_production_data()\n",
        "print(f\"\\nGenerated {len(data_files)} data files for ETL processing\")"
      ],
      "metadata": {
        "id": "generate-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ETL Pipeline - Built for Production\n",
        "\n",
        "Spark excels in production environments with built-in fault tolerance, monitoring, and distributed processing capabilities."
      ],
      "metadata": {
        "id": "spark-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session with production-like configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ProductionETL\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for data validation\n",
        "transaction_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), False),\n",
        "    StructField(\"customer_id\", StringType(), False),\n",
        "    StructField(\"product_id\", StringType(), False),\n",
        "    StructField(\"product_category\", StringType(), False),\n",
        "    StructField(\"order_total\", DoubleType(), True),\n",
        "    StructField(\"order_date\", TimestampType(), False),\n",
        "    StructField(\"processing_batch\", StringType(), False)\n",
        "])\n",
        "\n",
        "def spark_etl_pipeline():\n",
        "    \"\"\"Production ETL pipeline with Spark\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Read with schema enforcement and error handling\n",
        "        df = spark.read \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
        "            .schema(transaction_schema) \\\n",
        "            .csv(\"data/transactions/2025/01/*.csv\")\n",
        "        \n",
        "        print(f\"Read {df.count():,} total records\")\n",
        "        \n",
        "        # Data quality checks\n",
        "        invalid_customers = df.filter(col(\"customer_id\").startswith(\"INVALID\")).count()\n",
        "        null_totals = df.filter(col(\"order_total\").isNull()).count()\n",
        "        \n",
        "        print(f\"Data quality issues found:\")\n",
        "        print(f\"  - Invalid customers: {invalid_customers:,}\")\n",
        "        print(f\"  - Null order totals: {null_totals:,}\")\n",
        "        \n",
        "        # Clean data and add derived columns\n",
        "        cleaned_df = df.filter(\n",
        "            (~col(\"customer_id\").startswith(\"INVALID\")) & \n",
        "            (col(\"order_total\").isNotNull())\n",
        "        )\n",
        "        \n",
        "        # Complex transformations with built-in fault tolerance\n",
        "        processed = cleaned_df \\\n",
        "            .withColumn(\"processing_date\", current_date()) \\\n",
        "            .withColumn(\"is_weekend\", dayofweek(col(\"order_date\")).isin([1, 7])) \\\n",
        "            .withColumn(\"order_hour\", hour(col(\"order_date\"))) \\\n",
        "            .withColumn(\"order_month\", month(col(\"order_date\"))) \\\n",
        "            .groupBy(\"customer_id\", \"product_category\", \"processing_batch\") \\\n",
        "            .agg(\n",
        "                sum(\"order_total\").alias(\"category_total\"),\n",
        "                countDistinct(\"order_id\").alias(\"order_count\"),\n",
        "                avg(\"order_total\").alias(\"avg_order_value\"),\n",
        "                collect_list(\"product_id\").alias(\"products_purchased\"),\n",
        "                sum(when(col(\"is_weekend\"), 1).otherwise(0)).alias(\"weekend_orders\")\n",
        "            ) \\\n",
        "            .cache()  # Cache for multiple downstream operations\n",
        "        \n",
        "        result_count = processed.count()\n",
        "        print(f\"Processed {result_count:,} customer-category combinations\")\n",
        "        \n",
        "        # Write results with partitioning (simulated)\n",
        "        processed.coalesce(10).write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .option(\"compression\", \"snappy\") \\\n",
        "            .parquet(\"output/spark_customer_analytics\")\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        print(f\"Spark ETL completed in {processing_time:.2f} seconds\")\n",
        "        \n",
        "        return processed.toPandas().head(10)  # Return sample for display\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Spark ETL failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run Spark ETL\n",
        "spark_results = spark_etl_pipeline()\n",
        "print(\"\\nSample Spark ETL Results:\")\n",
        "spark_results"
      ],
      "metadata": {
        "id": "spark-etl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polars ETL Pipeline - High Performance with Custom Infrastructure\n",
        "\n",
        "Polars offers excellent performance but requires building custom fault tolerance and monitoring."
      ],
      "metadata": {
        "id": "polars-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polars_etl_pipeline():\n",
        "    \"\"\"High-performance ETL with custom error handling\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Define schema for validation\n",
        "    schema = {\n",
        "        \"order_id\": pl.Utf8,\n",
        "        \"customer_id\": pl.Utf8,\n",
        "        \"product_id\": pl.Utf8,\n",
        "        \"product_category\": pl.Utf8,\n",
        "        \"order_total\": pl.Float64,\n",
        "        \"order_date\": pl.Datetime,\n",
        "        \"processing_batch\": pl.Utf8\n",
        "    }\n",
        "    \n",
        "    # Read multiple files with custom error handling\n",
        "    dfs = []\n",
        "    failed_files = []\n",
        "    \n",
        "    for file_path in glob.glob(\"data/transactions/2025/01/*.csv\"):\n",
        "        try:\n",
        "            df = pl.read_csv(\n",
        "                file_path,\n",
        "                schema=schema,\n",
        "                try_parse_dates=True,\n",
        "                null_values=[\"NULL\", \"null\", \"\", \"NA\"]\n",
        "            )\n",
        "            dfs.append(df)\n",
        "            print(f\"Successfully processed {file_path}: {len(df):,} rows\")\n",
        "        except Exception as e:\n",
        "            failed_files.append(file_path)\n",
        "            logger.error(f\"Failed to process {file_path}: {e}\")\n",
        "    \n",
        "    if failed_files:\n",
        "        print(f\"Warning: {len(failed_files)} files failed to process\")\n",
        "    \n",
        "    # Concatenate all successful files\n",
        "    combined = pl.concat(dfs)\n",
        "    print(f\"Combined dataset: {len(combined):,} rows\")\n",
        "    \n",
        "    # Data quality checks and cleaning\n",
        "    invalid_customers = combined.filter(pl.col(\"customer_id\").str.starts_with(\"INVALID\")).height\n",
        "    null_totals = combined.filter(pl.col(\"order_total\").is_null()).height\n",
        "    \n",
        "    print(f\"Data quality issues:\")\n",
        "    print(f\"  - Invalid customers: {invalid_customers:,}\")\n",
        "    print(f\"  - Null order totals: {null_totals:,}\")\n",
        "    \n",
        "    # Clean and transform data\n",
        "    processed = (\n",
        "        combined\n",
        "        .filter(\n",
        "            (~pl.col(\"customer_id\").str.starts_with(\"INVALID\")) &\n",
        "            (pl.col(\"order_total\").is_not_null())\n",
        "        )\n",
        "        .with_columns([\n",
        "            pl.col(\"order_date\").dt.weekday().is_in([6, 7]).alias(\"is_weekend\"),\n",
        "            pl.col(\"order_date\").dt.hour().alias(\"order_hour\"),\n",
        "            pl.col(\"order_date\").dt.month().alias(\"order_month\")\n",
        "        ])\n",
        "        .group_by([\"customer_id\", \"product_category\", \"processing_batch\"])\n",
        "        .agg([\n",
        "            pl.col(\"order_total\").sum().alias(\"category_total\"),\n",
        "            pl.col(\"order_id\").n_unique().alias(\"order_count\"),\n",
        "            pl.col(\"order_total\").mean().alias(\"avg_order_value\"),\n",
        "            pl.col(\"product_id\").implode().alias(\"products_purchased\"),\n",
        "            pl.col(\"is_weekend\").sum().alias(\"weekend_orders\")\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    print(f\"Processed {len(processed):,} customer-category combinations\")\n",
        "    \n",
        "    # Write results (custom partitioning would be needed for production)\n",
        "    processed.write_parquet(\"output/polars_customer_analytics.parquet\")\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"Polars ETL completed in {processing_time:.2f} seconds\")\n",
        "    \n",
        "    return processed.head(10).to_pandas()  # Convert to pandas for display\n",
        "\n",
        "# Run Polars ETL\n",
        "polars_results = polars_etl_pipeline()\n",
        "print(\"\\nSample Polars ETL Results:\")\n",
        "polars_results"
      ],
      "metadata": {
        "id": "polars-etl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DuckDB ETL Pipeline - SQL-First Approach\n",
        "\n",
        "DuckDB provides excellent analytical performance with familiar SQL syntax, though with limited distributed processing."
      ],
      "metadata": {
        "id": "duckdb-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duckdb_etl_pipeline():\n",
        "    \"\"\"SQL-first ETL pipeline with DuckDB\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    conn = duckdb.connect(\":memory:\")\n",
        "    \n",
        "    try:\n",
        "        # Read all CSV files into a single table\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE raw_transactions AS \n",
        "            SELECT * FROM read_csv_auto('data/transactions/2025/01/*.csv')\n",
        "        \"\"\")\n",
        "        \n",
        "        total_rows = conn.execute(\"SELECT COUNT(*) FROM raw_transactions\").fetchone()[0]\n",
        "        print(f\"Loaded {total_rows:,} total records\")\n",
        "        \n",
        "        # Data quality analysis\n",
        "        quality_check = conn.execute(\"\"\"\n",
        "            SELECT \n",
        "                COUNT(*) FILTER (WHERE customer_id LIKE 'INVALID%') as invalid_customers,\n",
        "                COUNT(*) FILTER (WHERE order_total IS NULL) as null_totals,\n",
        "                COUNT(*) as total_records\n",
        "            FROM raw_transactions\n",
        "        \"\"\").fetchone()\n",
        "        \n",
        "        print(f\"Data quality issues:\")\n",
        "        print(f\"  - Invalid customers: {quality_check[0]:,}\")\n",
        "        print(f\"  - Null order totals: {quality_check[1]:,}\")\n",
        "        \n",
        "        # Complex ETL transformation with SQL\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE processed_analytics AS\n",
        "            SELECT \n",
        "                customer_id,\n",
        "                product_category,\n",
        "                processing_batch,\n",
        "                SUM(order_total) as category_total,\n",
        "                COUNT(DISTINCT order_id) as order_count,\n",
        "                AVG(order_total) as avg_order_value,\n",
        "                LIST(DISTINCT product_id) as products_purchased,\n",
        "                SUM(CASE WHEN EXTRACT(DOW FROM order_date) IN (0, 6) THEN 1 ELSE 0 END) as weekend_orders,\n",
        "                CURRENT_DATE as processing_date\n",
        "            FROM raw_transactions\n",
        "            WHERE customer_id NOT LIKE 'INVALID%' \n",
        "              AND order_total IS NOT NULL\n",
        "            GROUP BY customer_id, product_category, processing_batch\n",
        "        \"\"\")\n",
        "        \n",
        "        result_count = conn.execute(\"SELECT COUNT(*) FROM processed_analytics\").fetchone()[0]\n",
        "        print(f\"Processed {result_count:,} customer-category combinations\")\n",
        "        \n",
        "        # Export results\n",
        "        conn.execute(\"\"\"\n",
        "            COPY processed_analytics TO 'output/duckdb_customer_analytics.parquet' \n",
        "            (FORMAT PARQUET, COMPRESSION SNAPPY)\n",
        "        \"\"\")\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        print(f\"DuckDB ETL completed in {processing_time:.2f} seconds\")\n",
        "        \n",
        "        # Return sample results\n",
        "        return conn.execute(\"SELECT * FROM processed_analytics LIMIT 10\").df()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"DuckDB ETL failed: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# Run DuckDB ETL\n",
        "duckdb_results = duckdb_etl_pipeline()\n",
        "print(\"\\nSample DuckDB ETL Results:\")\n",
        "duckdb_results"
      ],
      "metadata": {
        "id": "duckdb-etl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas ETL Pipeline - Traditional Approach\n",
        "\n",
        "Pandas struggles with production ETL due to memory limitations and lack of built-in fault tolerance, but we'll demonstrate the approach."
      ],
      "metadata": {
        "id": "pandas-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_etl_pipeline():\n",
        "    \"\"\"Traditional ETL with pandas - limited scalability\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Read files with basic error handling\n",
        "        dfs = []\n",
        "        failed_files = []\n",
        "        \n",
        "        for file_path in glob.glob(\"data/transactions/2025/01/*.csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(\n",
        "                    file_path,\n",
        "                    parse_dates=['order_date'],\n",
        "                    dtype={\n",
        "                        'order_id': 'string',\n",
        "                        'customer_id': 'string',\n",
        "                        'product_id': 'string',\n",
        "                        'product_category': 'string',\n",
        "                        'processing_batch': 'string'\n",
        "                    }\n",
        "                )\n",
        "                dfs.append(df)\n",
        "                print(f\"Loaded {file_path}: {len(df):,} rows\")\n",
        "            except Exception as e:\n",
        "                failed_files.append(file_path)\n",
        "                logger.error(f\"Failed to process {file_path}: {e}\")\n",
        "        \n",
        "        if failed_files:\n",
        "            print(f\"Warning: {len(failed_files)} files failed\")\n",
        "        \n",
        "        # Concatenate (memory intensive)\n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"Combined dataset: {len(combined):,} rows\")\n",
        "        print(f\"Memory usage: {combined.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "        \n",
        "        # Data quality checks\n",
        "        invalid_customers = combined[combined['customer_id'].str.startswith('INVALID', na=False)].shape[0]\n",
        "        null_totals = combined['order_total'].isnull().sum()\n",
        "        \n",
        "        print(f\"Data quality issues:\")\n",
        "        print(f\"  - Invalid customers: {invalid_customers:,}\")\n",
        "        print(f\"  - Null order totals: {null_totals:,}\")\n",
        "        \n",
        "        # Clean and transform\n",
        "        cleaned = combined[\n",
        "            (~combined['customer_id'].str.startswith('INVALID', na=False)) &\n",
        "            (combined['order_total'].notna())\n",
        "        ].copy()\n",
        "        \n",
        "        # Add derived columns\n",
        "        cleaned['is_weekend'] = cleaned['order_date'].dt.dayofweek.isin([5, 6])\n",
        "        cleaned['order_hour'] = cleaned['order_date'].dt.hour\n",
        "        cleaned['order_month'] = cleaned['order_date'].dt.month\n",
        "        \n",
        "        # Aggregation (memory intensive for large datasets)\n",
        "        processed = (\n",
        "            cleaned.groupby(['customer_id', 'product_category', 'processing_batch'])\n",
        "            .agg({\n",
        "                'order_total': ['sum', 'mean'],\n",
        "                'order_id': 'nunique',\n",
        "                'product_id': lambda x: list(x.unique()),\n",
        "                'is_weekend': 'sum'\n",
        "            })\n",
        "            .round(2)\n",
        "        )\n",
        "        \n",
        "        # Flatten column names\n",
        "        processed.columns = ['category_total', 'avg_order_value', 'order_count', 'products_purchased', 'weekend_orders']\n",
        "        processed = processed.reset_index()\n",
        "        \n",
        "        print(f\"Processed {len(processed):,} customer-category combinations\")\n",
        "        \n",
        "        # Save results\n",
        "        processed.to_parquet(\"output/pandas_customer_analytics.parquet\", compression='snappy')\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        print(f\"Pandas ETL completed in {processing_time:.2f} seconds\")\n",
        "        \n",
        "        return processed.head(10)\n",
        "        \n",
        "    except MemoryError:\n",
        "        print(\"ERROR: Pandas ran out of memory - this is why it's not suitable for production ETL at scale\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pandas ETL failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run Pandas ETL (may fail due to memory constraints)\n",
        "try:\n",
        "    pandas_results = pandas_etl_pipeline()\n",
        "    print(\"\\nSample Pandas ETL Results:\")\n",
        "    print(pandas_results)\n",
        "except Exception as e:\n",
        "    print(f\"Pandas ETL failed as expected: {e}\")"
      ],
      "metadata": {
        "id": "pandas-etl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Comparison and Analysis\n",
        "\n",
        "Let's compare the tools across key production ETL criteria:"
      ],
      "metadata": {
        "id": "comparison-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance and capability comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create comparison matrix\n",
        "comparison_data = {\n",
        "    'Tool': ['Spark', 'Polars', 'DuckDB', 'Pandas'],\n",
        "    'Fault Tolerance': [5, 2, 3, 1],  # 1-5 scale\n",
        "    'Scalability': [5, 4, 3, 1],\n",
        "    'Performance': [4, 5, 4, 2],\n",
        "    'Monitoring': [5, 2, 2, 1],\n",
        "    'Ecosystem': [5, 3, 3, 5],\n",
        "    'Production Ready': [5, 3, 4, 2]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"Production ETL Capability Comparison (1-5 scale):\")\n",
        "print(comparison_df.set_index('Tool'))\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Radar chart for capabilities\n",
        "categories = ['Fault Tolerance', 'Scalability', 'Performance', 'Monitoring', 'Ecosystem', 'Production Ready']\n",
        "tools = comparison_df['Tool'].tolist()\n",
        "\n",
        "# Bar chart for overall production readiness\n",
        "overall_scores = comparison_df.set_index('Tool').mean(axis=1)\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "bars = ax1.bar(overall_scores.index, overall_scores.values, color=colors)\n",
        "ax1.set_title('Overall Production ETL Readiness Score')\n",
        "ax1.set_ylabel('Average Score (1-5)')\n",
        "ax1.set_ylim(0, 5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars, overall_scores.values):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             f'{score:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# Capability breakdown\n",
        "capability_matrix = comparison_df.set_index('Tool').T\n",
        "im = ax2.imshow(capability_matrix.values, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
        "ax2.set_xticks(range(len(tools)))\n",
        "ax2.set_xticklabels(tools)\n",
        "ax2.set_yticks(range(len(categories)))\n",
        "ax2.set_yticklabels(categories)\n",
        "ax2.set_title('Detailed Capability Heatmap')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(categories)):\n",
        "    for j in range(len(tools)):\n",
        "        ax2.text(j, i, capability_matrix.iloc[i, j], ha='center', va='center', \n",
        "                color='white' if capability_matrix.iloc[i, j] < 3 else 'black')\n",
        "\n",
        "plt.colorbar(im, ax=ax2, label='Capability Score (1-5)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== SCENARIO 2 CONCLUSIONS ===\")\n",
        "print(\"\\nðŸ† WINNER: Spark for Enterprise ETL\")\n",
        "print(\"   - Built-in fault tolerance and error recovery\")\n",
        "print(\"   - Comprehensive monitoring and logging\")\n",
        "print(\"   - Mature ecosystem with extensive tooling\")\n",
        "print(\"   - Handles schema evolution and data validation\")\n",
        "\n",
        "print(\"\\nðŸ¥ˆ RUNNER-UP: Polars for High-Performance Custom ETL\")\n",
        "print(\"   - Excellent performance for batch processing\")\n",
        "print(\"   - Lower resource requirements than Spark\")\n",
        "print(\"   - Requires building custom infrastructure\")\n",
        "\n",
        "print(\"\\nðŸ“Š DuckDB: Good for Medium-Scale Analytics ETL\")\n",
        "print(\"   - SQL-first approach familiar to analysts\")\n",
        "print(\"   - Excellent analytical performance\")\n",
        "print(\"   - Limited distributed processing capabilities\")\n",
        "\n",
        "print(\"\\nâš ï¸  Pandas: Not Suitable for Production ETL\")\n",
        "print(\"   - Memory limitations prevent scaling\")\n",
        "print(\"   - No built-in fault tolerance\")\n",
        "print(\"   - Single-threaded processing\")"
      ],
      "metadata": {
        "id": "comparison-analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped successfully\")"
      ],
      "metadata": {
        "id": "cleanup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}